{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":16295,"databundleVersionId":1099992,"sourceType":"competition"},{"sourceId":288210668,"sourceType":"kernelVersion"},{"sourceId":697401,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":528984,"modelId":543014}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install evaluate\n!pip install torch\n!pip install accelerate\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:54:53.298366Z","iopub.execute_input":"2025-12-24T11:54:53.298979Z","iopub.status.idle":"2025-12-24T11:55:12.411598Z","shell.execute_reply.started":"2025-12-24T11:54:53.298948Z","shell.execute_reply":"2025-12-24T11:55:12.410598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:12.704644Z","iopub.execute_input":"2025-12-24T11:55:12.704915Z","iopub.status.idle":"2025-12-24T11:55:12.718945Z","shell.execute_reply.started":"2025-12-24T11:55:12.704895Z","shell.execute_reply":"2025-12-24T11:55:12.718252Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# Read the Input CSV\nThen use info and describe to see  dataset info and stats like no of non rull rows etc","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\n#print(df.head())\nprint(df.info())\nprint(df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:12.418583Z","iopub.execute_input":"2025-12-24T11:55:12.418850Z","iopub.status.idle":"2025-12-24T11:55:12.692969Z","shell.execute_reply.started":"2025-12-24T11:55:12.418819Z","shell.execute_reply":"2025-12-24T11:55:12.692238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"sentiment\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:12.694798Z","iopub.execute_input":"2025-12-24T11:55:12.695347Z","iopub.status.idle":"2025-12-24T11:55:12.702861Z","shell.execute_reply.started":"2025-12-24T11:55:12.695323Z","shell.execute_reply":"2025-12-24T11:55:12.702203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Drop rows that have NaN values\nIf the 'text' column has NaN values the we drop the row","metadata":{}},{"cell_type":"code","source":"df=df.dropna(subset=['text'])\nprint(df.info())\nprint(df.describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:12.719830Z","iopub.execute_input":"2025-12-24T11:55:12.720096Z","iopub.status.idle":"2025-12-24T11:55:12.794491Z","shell.execute_reply.started":"2025-12-24T11:55:12.720066Z","shell.execute_reply":"2025-12-24T11:55:12.793874Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert Pandas Dataframe to HuggingFace Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\ndataset=Dataset.from_pandas(df)\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:12.795329Z","iopub.execute_input":"2025-12-24T11:55:12.795700Z","iopub.status.idle":"2025-12-24T11:55:14.211028Z","shell.execute_reply.started":"2025-12-24T11:55:12.795667Z","shell.execute_reply":"2025-12-24T11:55:14.210344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split the Dataset into train and test","metadata":{}},{"cell_type":"code","source":"dataset=dataset.train_test_split(test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:14.211982Z","iopub.execute_input":"2025-12-24T11:55:14.212484Z","iopub.status.idle":"2025-12-24T11:55:14.249174Z","shell.execute_reply.started":"2025-12-24T11:55:14.212447Z","shell.execute_reply":"2025-12-24T11:55:14.248684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset[\"train\"])\nprint(dataset[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:09:18.451395Z","iopub.status.idle":"2025-12-24T12:09:18.451790Z","shell.execute_reply.started":"2025-12-24T12:09:18.451607Z","shell.execute_reply":"2025-12-24T12:09:18.451631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import and Load the Toeknizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:14.255928Z","iopub.execute_input":"2025-12-24T11:55:14.256145Z","iopub.status.idle":"2025-12-24T11:55:16.994382Z","shell.execute_reply.started":"2025-12-24T11:55:14.256124Z","shell.execute_reply":"2025-12-24T11:55:16.993681Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create mappings from label to ids and ids to label","metadata":{}},{"cell_type":"code","source":"label2id={\n    \"negative\": 0,\n    \"neutral\": 1,\n    \"positive\": 2\n}\n\nid2label={\n    0: \"negative\",\n    1: \"neutral\",\n    2: \"positive\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:16.995425Z","iopub.execute_input":"2025-12-24T11:55:16.995893Z","iopub.status.idle":"2025-12-24T11:55:17.001086Z","shell.execute_reply.started":"2025-12-24T11:55:16.995858Z","shell.execute_reply":"2025-12-24T11:55:17.000299Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* Map every single row to respective id.\n* 'example' is each row from the dataset.\n* We create a 'labels' column and add the mapped ids in that column.\n* The .map() method calls the argument function(encode_labels() in this case) for every row in dataset.\n\n\n","metadata":{}},{"cell_type":"code","source":"def encode_labels(example):\n    example[\"labels\"]=label2id[example[\"sentiment\"]]\n    return example\n\ndataset=dataset.map(encode_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:17.002122Z","iopub.execute_input":"2025-12-24T11:55:17.002395Z","iopub.status.idle":"2025-12-24T11:55:19.578417Z","shell.execute_reply.started":"2025-12-24T11:55:17.002369Z","shell.execute_reply":"2025-12-24T11:55:19.577745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset)\nprint(dataset[\"train\"][\"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:19.580293Z","iopub.execute_input":"2025-12-24T11:55:19.580607Z","iopub.status.idle":"2025-12-24T11:55:19.585449Z","shell.execute_reply.started":"2025-12-24T11:55:19.580571Z","shell.execute_reply":"2025-12-24T11:55:19.584799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenize the Dataset\n* 'batch' is a dictionry of lists: {\"text\":[\"Tweet1\",\"Tweet2\",...], \"label\":[1, 2, 0, ...]}\n* Therefore batch[text] is a list of strings-[\"Tweet1\",\"Tweet2\",...]\n* Tokenizer returns a dictionary: {\"input_ids\": [....], \"attention_mask\": [...]}\n* input_ids -> each no maps to a token in the input text\n* attention_mask([1,1,1,0,0,0]) -> model uses this to ignore padding token etc\n\n\n  ","metadata":{}},{"cell_type":"code","source":"def tokenize_function(batch):\n    return tokenizer(\n        batch[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:19.586436Z","iopub.execute_input":"2025-12-24T11:55:19.586841Z","iopub.status.idle":"2025-12-24T11:55:19.643930Z","shell.execute_reply.started":"2025-12-24T11:55:19.586818Z","shell.execute_reply":"2025-12-24T11:55:19.643374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_dataset=dataset.map(tokenize_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:21.068677Z","iopub.execute_input":"2025-12-24T11:55:21.068962Z","iopub.status.idle":"2025-12-24T11:55:23.513592Z","shell.execute_reply.started":"2025-12-24T11:55:21.068937Z","shell.execute_reply":"2025-12-24T11:55:23.512851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Important to set the format of the tensors are torch or else it will cause problems","metadata":{}},{"cell_type":"code","source":"tokenized_dataset.set_format(\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:25.309531Z","iopub.execute_input":"2025-12-24T11:55:25.310162Z","iopub.status.idle":"2025-12-24T11:55:25.314508Z","shell.execute_reply.started":"2025-12-24T11:55:25.310133Z","shell.execute_reply":"2025-12-24T11:55:25.313827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import and Load the model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel=AutoModelForSequenceClassification.from_pretrained(\n    \"xlm-roberta-base\",\n    num_labels=3,\n    label2id=label2id,\n    id2label=id2label\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:55:25.859469Z","iopub.execute_input":"2025-12-24T11:55:25.860178Z","iopub.status.idle":"2025-12-24T11:55:55.048660Z","shell.execute_reply.started":"2025-12-24T11:55:25.860144Z","shell.execute_reply":"2025-12-24T11:55:55.047631Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set the training arguments","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=8,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    logging_steps=100,\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:56:25.876814Z","iopub.execute_input":"2025-12-24T11:56:25.877629Z","iopub.status.idle":"2025-12-24T11:56:25.909901Z","shell.execute_reply.started":"2025-12-24T11:56:25.877584Z","shell.execute_reply":"2025-12-24T11:56:25.909171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Set the Evaluation Metrics\n* logits -> contains the raw scores from each classification \n* predcition -> aggregate raw logit scores to softmax probabilities\n* metric.compute() -> aggregate f1 score for each sample,\n* F1 score -> we will calculate the F1 score for each class and then take weighted average","metadata":{}},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:56:28.123071Z","iopub.execute_input":"2025-12-24T11:56:28.123627Z","iopub.status.idle":"2025-12-24T11:56:28.662106Z","shell.execute_reply.started":"2025-12-24T11:56:28.123586Z","shell.execute_reply":"2025-12-24T11:56:28.661330Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Start the training","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:56:29.949925Z","iopub.execute_input":"2025-12-24T11:56:29.950616Z","iopub.status.idle":"2025-12-24T11:56:29.979189Z","shell.execute_reply.started":"2025-12-24T11:56:29.950588Z","shell.execute_reply":"2025-12-24T11:56:29.978435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:09:56.078518Z","iopub.execute_input":"2025-12-24T12:09:56.079217Z","iopub.status.idle":"2025-12-24T12:34:04.113109Z","shell.execute_reply.started":"2025-12-24T12:09:56.079186Z","shell.execute_reply":"2025-12-24T12:34:04.112287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T12:38:32.239233Z","iopub.execute_input":"2025-12-24T12:38:32.239567Z","iopub.status.idle":"2025-12-24T12:39:23.156855Z","shell.execute_reply.started":"2025-12-24T12:38:32.239526Z","shell.execute_reply":"2025-12-24T12:39:23.156158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save the Trained Model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/xlm_roberta_tone_model\")\ntokenizer.save_pretrained(\"/kaggle/working/xlm_roberta_tone_model\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r xlm_roberta_tone_model.zip xlm_roberta_tone_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:04:24.356909Z","iopub.execute_input":"2025-12-24T13:04:24.357660Z","iopub.status.idle":"2025-12-24T13:05:42.622409Z","shell.execute_reply.started":"2025-12-24T13:04:24.357621Z","shell.execute_reply":"2025-12-24T13:05:42.621317Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the saved model for inference","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nimport torch.nn.functional as F\n\nMODEL_PATH=\"/kaggle/input/xlm-roberta-tweetdetection/pytorch/default/1/xlm_roberta_tone_model\"\n\nsaved_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nsaved_model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n\nsaved_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:22:51.117556Z","iopub.execute_input":"2025-12-27T08:22:51.117809Z","iopub.status.idle":"2025-12-27T08:23:34.443082Z","shell.execute_reply.started":"2025-12-27T08:22:51.117786Z","shell.execute_reply":"2025-12-27T08:23:34.441953Z"}},"outputs":[{"name":"stderr","text":"2025-12-27 08:23:14.025579: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766823794.354662      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766823794.450445      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766823795.240265      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766823795.240336      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766823795.240339      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766823795.240342      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"XLMRobertaForSequenceClassification(\n  (roberta): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): XLMRobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nsaved_model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:24:53.094458Z","iopub.execute_input":"2025-12-27T08:24:53.094970Z","iopub.status.idle":"2025-12-27T08:24:53.109255Z","shell.execute_reply.started":"2025-12-27T08:24:53.094909Z","shell.execute_reply":"2025-12-27T08:24:53.108304Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"XLMRobertaForSequenceClassification(\n  (roberta): XLMRobertaModel(\n    (embeddings): XLMRobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): XLMRobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x XLMRobertaLayer(\n          (attention): XLMRobertaAttention(\n            (self): XLMRobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): XLMRobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): XLMRobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): XLMRobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): XLMRobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def predict_sentiment(text):\n    inputs = saved_tokenizer(\n        text,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=128\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        logits = saved_model(**inputs).logits\n\n    pred_id = torch.argmax(logits, dim=1).item()\n    return saved_model.config.id2label[pred_id]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:24:56.149329Z","iopub.execute_input":"2025-12-27T08:24:56.150466Z","iopub.status.idle":"2025-12-27T08:24:56.158021Z","shell.execute_reply.started":"2025-12-27T08:24:56.150429Z","shell.execute_reply":"2025-12-27T08:24:56.156763Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Use the below cell to manually check the output","metadata":{}},{"cell_type":"code","source":"predict_sentiment(\"I am not coming there\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T08:24:59.298970Z","iopub.execute_input":"2025-12-27T08:24:59.299788Z","iopub.status.idle":"2025-12-27T08:25:01.660862Z","shell.execute_reply.started":"2025-12-27T08:24:59.299753Z","shell.execute_reply":"2025-12-27T08:25:01.659901Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'neutral'"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Generate a submission.csv file for the Competition submission","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\ntest_dataset=Dataset.from_pandas(df)\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T13:48:45.431125Z","iopub.execute_input":"2025-12-24T13:48:45.431680Z","iopub.status.idle":"2025-12-24T13:48:45.456184Z","shell.execute_reply.started":"2025-12-24T13:48:45.431652Z","shell.execute_reply":"2025-12-24T13:48:45.455599Z"}},"outputs":[{"name":"stdout","text":"          textID                                               text sentiment\n0     f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n1     96d74cb729   Shanghai is also really exciting (precisely -...  positive\n2     eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n3     01082688c6                                        happy bday!  positive\n4     33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive\n...          ...                                                ...       ...\n3529  e5f0e6ef4b  its at 3 am, im very tired but i can`t sleep  ...  negative\n3530  416863ce47  All alone in this old house again.  Thanks for...  positive\n3531  6332da480c   I know what you mean. My little dog is sinkin...  negative\n3532  df1baec676  _sutra what is your next youtube video gonna b...  positive\n3533  469e15c5a8   http://twitpic.com/4woj2 - omgssh  ang cute n...  positive\n\n[3534 rows x 3 columns]\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}